{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating percentiles for TensorFlow model input features\n",
    "\n",
    "The current TensorFlow model uses histogram-like percentile features, which are kind of a continuous version of one-hot features.\n",
    "\n",
    "For example, if key cutoff points are `[-3, 1, 0, 2, 10]`, we might encode a value `x` as `sigma((x - cutoff) / scale)`. If `sigma` is the sigmoid function, `x = 0.1`, and `scale = 0.1`, then we'd get `[1, 1, 0.73, 0, 0]`, in other words `x` is definitely above the first 2 points, mostly above the third, and below the fourth and fifth. If we increase `scale` to `2.0`, then values are less discrete: `[0.82, 0.63, 0.51, 0.28, 0.01]`.\n",
    "\n",
    "This notebook generates appropriate cutoff points for these, to reflect most data encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different options for soft-onehot function.\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.linspace(-10, 10, 100)\n",
    "cutoff = 1.0\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "scale = 2.0\n",
    "logit = (x - cutoff) / scale\n",
    "plt.plot(x, sigmoid(logit))\n",
    "plt.plot(x, np.exp(- logit * logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LCS = 10_000  # key parameter, turn it down if you want this notebook to finish faster.\n",
    "\n",
    "# Settings determining type of features extracted.\n",
    "window_size = 10\n",
    "band_time_diff = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from justice.datasets import plasticc_data\n",
    "\n",
    "source = plasticc_data.PlasticcBcolzSource.get_default()\n",
    "bcolz_source = plasticc_data.PlasticcBcolzSource.get_default()\n",
    "meta_table = bcolz_source.get_table('test_set_metadata')\n",
    "%time all_ids = meta_table['object_id'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "sample_ids = random.Random(828372).sample(list(all_ids), NUM_LCS)\n",
    "\n",
    "lcs = []\n",
    "_chunk_sz = 100\n",
    "for start in range(0, len(sample_ids), _chunk_sz):\n",
    "    lcs.extend(plasticc_data.PlasticcDatasetLC.bcolz_get_lcs_by_obj_ids(\n",
    "        bcolz_source=source,\n",
    "        dataset=\"test_set\",\n",
    "        obj_ids=sample_ids[start:start + _chunk_sz]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from justice.features import band_settings_params\n",
    "from justice.features import dense_extracted_features\n",
    "from justice.features import feature_combinators\n",
    "from justice.features import metadata_features\n",
    "from justice.features import per_point_dataset\n",
    "from justice.features import raw_value_features\n",
    "\n",
    "batch_size = 32\n",
    "rve = raw_value_features.RawValueExtractor(\n",
    "    window_size=window_size,\n",
    "    band_settings=band_settings_params.BandSettings(lcs[0].expected_bands)\n",
    ")\n",
    "mve = metadata_features.MetadataValueExtractor()\n",
    "data_gen = per_point_dataset.PerPointDatasetGenerator(\n",
    "    extract_fcn=feature_combinators.combine([rve.extract, mve.extract]),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "def input_fn():\n",
    "    return data_gen.make_dataset_lcs(lcs)\n",
    "\n",
    "def per_band_model_fn(band_features, params):\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    window_size = params[\"window_size\"]\n",
    "    wf = dense_extracted_features.WindowFeatures(\n",
    "        band_features, batch_size=batch_size, window_size=window_size, band_time_diff=band_time_diff)\n",
    "    dflux_dt = wf.dflux_dt(clip_magnitude=None)\n",
    "    init_layer = dense_extracted_features.initial_layer(wf, include_flux_and_time=True)\n",
    "    init_layer_masked = wf.masked(init_layer, value_if_masked=0, expected_extra_dims=[3])\n",
    "    return {\n",
    "        \"initial_layer\": init_layer_masked,\n",
    "        \"in_window\": wf.in_window,\n",
    "    }\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    band_settings = band_settings_params.BandSettings.from_params(params)\n",
    "    per_band_data = band_settings.per_band_sub_model_fn(\n",
    "        per_band_model_fn, features, params=params\n",
    "    )\n",
    "    predictions = {\n",
    "        'band_{}.{}'.format(band, name): tensor\n",
    "        for band, tensor_dict in zip(band_settings.bands, per_band_data)\n",
    "        for name, tensor in tensor_dict.items()\n",
    "    }\n",
    "    predictions['time'] = features['time']\n",
    "    predictions['object_id'] = features['object_id']\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, predictions=predictions, loss=tf.constant(0.0), train_op=tf.no_op()\n",
    "    )\n",
    "\n",
    "params = {\n",
    "    'batch_size': batch_size,\n",
    "    'window_size': window_size,\n",
    "    'flux_scale_epsilon': 0.5,\n",
    "    'lc_bands': lcs[0].expected_bands,\n",
    "}\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    params=params\n",
    ")\n",
    "predictions = list(estimator.predict(input_fn=input_fn, yield_single_examples=True))\n",
    "print(f\"Got {len(predictions)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_df(band):\n",
    "    arrays = [x[f\"band_{band}.initial_layer\"] for x in predictions if x[f\"band_{band}.in_window\"]]\n",
    "    return pd.DataFrame(np.concatenate(arrays, axis=0), columns=[\"dflux_dt\", \"dflux\", \"dtime\"])\n",
    "df = get_values_df(lcs[0].expected_bands[0])\n",
    "df.hist('dflux_dt', bins=32)\n",
    "df.hist('dflux', bins=32)\n",
    "df.hist('dtime', bins=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Really messy code to get a histogram with mostly-unique bins.\n",
    "\n",
    "Because we want fixed-size arrays for TensorFlow code, we want a set of e.g. 32 unique cutoff points that reflect a good distribution of cutoffs. However its is really messy, because there tend to be strong peaks in the histogram which are repeated frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import scipy.optimize\n",
    "\n",
    "def _some_duplicates(non_unique, unique, num_desired):\n",
    "    to_duplicate_candidates = non_unique.tolist()\n",
    "    for x in unique:\n",
    "        to_duplicate_candidates.remove(x)\n",
    "    unique = unique.tolist()\n",
    "    while len(unique) < num_desired:\n",
    "        assert len(unique) <= num_desired\n",
    "        to_duplicate = random.choice(to_duplicate_candidates)\n",
    "        unique.insert(unique.index(to_duplicate), to_duplicate)\n",
    "    return unique\n",
    "\n",
    "def unique_percentiles(array, num_desired):\n",
    "    partition_size = 100.0 / num_desired\n",
    "    epsilon = 0.05 * partition_size\n",
    "    \n",
    "    solution = None\n",
    "    optimal_solution = None\n",
    "\n",
    "    def _actual_unique(vals):\n",
    "        nonlocal solution, optimal_solution\n",
    "        if optimal_solution is not None:\n",
    "            return 0  # stop optimization, or at least return quickly\n",
    "        num_points_base, perturb = vals\n",
    "        num_points = int(round(num_desired * num_points_base))\n",
    "        perturb = abs(perturb)\n",
    "        q = np.linspace(0, 100, int(num_points))\n",
    "        rng = np.random.RandomState(int(1e6 * perturb))\n",
    "        noise = rng.normal(loc=0, scale=min(1.0, 10 * perturb) * epsilon, size=q.shape)\n",
    "        noise[0] = 0\n",
    "        noise[-1] = 0\n",
    "        q += noise\n",
    "        non_unique = np.percentile(array, q=q, interpolation='linear')\n",
    "        unique = np.unique(non_unique)\n",
    "        result = abs(num_desired - len(unique))\n",
    "        if num_desired == len(unique):\n",
    "            optimal_solution = unique\n",
    "        elif len(unique) <= num_desired <= len(unique) + 1:\n",
    "            solution = _some_duplicates(non_unique, unique, num_desired)\n",
    "        return (4 if len(unique) > num_desired else 1) * result + perturb\n",
    "    \n",
    "    res = scipy.optimize.minimize(\n",
    "        _actual_unique,\n",
    "        x0=[1.0, 0.1],\n",
    "        options={'maxiter': 1000, 'rhobeg': 0.3},\n",
    "        tol=1e-6,\n",
    "        method='COBYLA')\n",
    "    if optimal_solution is None and solution is None:\n",
    "        raise ValueError(f\"Could not find deduplicated percentiles!\")\n",
    "    return optimal_solution if optimal_solution is not None else solution\n",
    "\n",
    "desired_num_cutoffs = 32\n",
    "all_solutions = []\n",
    "for band in lcs[0].expected_bands:\n",
    "    df = get_values_df(band)\n",
    "    for i, column in enumerate(df.columns):\n",
    "        print(band, column)\n",
    "        percentiles = np.array(unique_percentiles(df[column], desired_num_cutoffs), dtype=np.float32)\n",
    "        median_scale = np.median(percentiles[1:] - percentiles[:-1])\n",
    "        all_solutions.append({\n",
    "            'band': band,\n",
    "            'column_index': i,\n",
    "            'column': column,\n",
    "            'median_scale': float(median_scale),\n",
    "            'cutoffs': percentiles,\n",
    "        })\n",
    "\n",
    "with_settings = {\n",
    "    'window_size': window_size,\n",
    "    'band_time_diff': band_time_diff,\n",
    "    'desired_num_cutoffs': desired_num_cutoffs,\n",
    "    'solutions': all_solutions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to nicely-formatted JSON\n",
    "\n",
    "Writes numpy arrays as strings, then rewrites those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "from justice import path_util\n",
    "\n",
    "class ArrayPreEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return \"<<<<{}>>>>\".format(\", \".join(f\"{x:.8f}\" for x in obj.tolist()))\n",
    "        else:\n",
    "            print(obj)\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def _encode(x):\n",
    "    result = json.dumps(x, indent=2, cls=ArrayPreEncoder).replace('\"<<<<', '[').replace('>>>>\"', ']')\n",
    "    json.loads(result)  # error if not decodable\n",
    "    return result\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "path = path_util.data_dir / 'tf_align_model' / 'feature_extraction' / (\n",
    "    f\"cutoffs__window_sz-{window_size}__{now.year:04d}-{now.month:02d}-{now.day:02d}.json\")\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(str(path), 'w') as f:\n",
    "    f.write(_encode(with_settings))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
