{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Alex Malz, David Mykytyn, Zora Tung_\n",
    "\n",
    "This is a sandbox for developing an unsupervised classifier of astronomical lightcurves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import corner\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some mock data\n",
    "\n",
    "We may need to preprocess to keep it reasonable, constraints on delta/stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justice import simulate as sim\n",
    "from justice import summarize as summ\n",
    "from justice import visualize as vis\n",
    "from justice import xform\n",
    "from justice import lightcurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of LC shapes might we have to worry about physically?  For now, just one transient (Gaussian) and one variable (sinusoid).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lightcurves are sampled irregularly/sparsely in x and have observational errors/noise on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, ready to make some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glc = sim.TestLC.make_easy_gauss()\n",
    "slc = sim.TestLC.make_easy_sine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_lcs([glc, slc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The overall strategy\n",
    "\n",
    "We want to test the hypothesis that two lightcurves are noisy/sparse/irregular observations of the same object, under some permitted (afine) transformations.  Then we want to do clustering in the space of goodness-of-fit/consistency measure and the parameters of those transformations to identify classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularize in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regx(lca0, lcb0, lcc):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permitted transformations\n",
    "\n",
    "* shiftx\n",
    "* stretchx\n",
    "* shifty\n",
    "* stretchy\n",
    "* (cross-talk between bands)\n",
    "\n",
    "\n",
    "also adjust error bars\n",
    "\n",
    "should we first identify zero point in time to not have to worry as much about the arclength optimization? \n",
    "could we do that with fourier transform first?\n",
    "start with large grid, then refine grid, dayish scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce to summary statistics (consistency metric)\n",
    "\n",
    "Contenders:\n",
    "\n",
    "* periodogram -- identify periodicity and stochastic noise levels, still okay to initially divide transient from variable\n",
    "* flux per time bins -- trends keeping bin size constant but changing bin ends, i.e. moving window\n",
    "* abs/percent change in color and total flux/magnitude\n",
    "\n",
    "find MAP/MLE of p(A = B | lc_A, lc_B)\n",
    "marginalize over shift/stretch params\n",
    "\n",
    "Regularization is going to be really hard!\n",
    "\n",
    "connect the dots is taking an arc length\n",
    "\n",
    "could use the gaussian error bars to get probability that new hypothesis point lies on original line?\n",
    "\n",
    "Random ideas: gaussian process kernels based on training set, get probability that connect-the-dots is drawn from that distribution; but, we know training set will be biased/incomplete, so will have to be able to adapt kernel, use some kind of exporation of space of kernels (gradient descent, genetic algorithm, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the arclength.  If the two lightcurves came from the same object, then their arclengths should be comparable to one another (if over the same range).  Merging the lightcurves should not significantly affect the total arclength.  We could also consider an area between curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing an optimization to find the transformation parameters minimizing the arclength ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff = xform.Aff(50., 1., 1., 1.5)\n",
    "glc2 = xform.transform(glc, aff)\n",
    "\n",
    "aff = summ.opt_arclen(glc, glc2, vb=False)\n",
    "print (aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff2 = summ.opt_gp(glc, glc2, vb=False, options={'maxiter':1009})\n",
    "print (aff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in zip(aff,aff2):\n",
    "    print (x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try just doing this with merging and shifting for now and test it when the lightcurves have the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.plot_arclen_res(glc, glc2, aff)\n",
    "vis.plot_arclen_res(glc, glc2, aff2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glc3 = lightcurve.merge(glc, xform.transform(glc2, aff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPy Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_cadence = np.vstack((np.arange(0., 1000., 10.),np.arange(0., 1000., 10.))).T\n",
    "pred, like = summ.pred_gp(glc3, def_cadence)\n",
    "print (pred.x.shape, like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note to self: Turn this into plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPy.models.gp_regression.GPRegression(np.array(glc3.x), np.array(glc3.y), normalizer=True)\n",
    "m.optimize()\n",
    "print (glc3.yerr.shape)\n",
    "newLC = lightcurve.LC(m.X, m.Y, glc3.yerr)\n",
    "def_cadence = np.vstack((np.arange(0., 1000., 10.),np.arange(0., 1000., 10.))).T\n",
    "quants = m.predict_quantiles(def_cadence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowers0, lowers1 = zip(*quants[0])\n",
    "uppers0, uppers1 = zip(*quants[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(glc3.x[:,0],glc3.y[:,0],marker='.')\n",
    "plt.scatter(glc3.x[:,1],glc3.y[:,1],marker='.')\n",
    "plt.plot(def_cadence[:,0],lowers0, 'k-', alpha=.5)\n",
    "plt.plot(def_cadence[:,0],uppers0, 'k-', alpha=.5)\n",
    "plt.plot(def_cadence[:,1],lowers1, 'r-', alpha=.5)\n",
    "plt.plot(def_cadence[:,1],uppers1, 'r-', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(glc3.x[:,1],glc3.y[:,1])\n",
    "plt.plot(def_cadence[:,1],lowers1, 'k-', alpha=.5)\n",
    "plt.plot(def_cadence[:,1],uppers1, 'k-', alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_lcs(newLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently broken after here due to multidim changes!!\n",
    "\n",
    "### Gaussian processes as an alternative to the arclength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from george import kernels\n",
    "# help(kernels.ExpSquaredKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glc4 = lightcurve.LC(np.vstack(glc3.x).T,np.vstack(glc3.y).T,np.vstack(glc3.yerr).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is definitely not the right kernel for the job!\n",
    "def_cadence = np.vstack((np.arange(0., 1000., 10.)),)\n",
    "kernel = np.var(glc4.y) * kernels.ExpSquaredKernel(10.)\n",
    "lcf, fin_like = summ.fit_gp(kernel, glc4, def_cadence)\n",
    "\n",
    "vis.plot_gp_res(glc4, lcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broken after this point!  Will turn the rest into scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do this many times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obj = 10\n",
    "cls_models = [make_gauss, make_sine]\n",
    "cls_params = [{'scale': 10., 'loc': 100., 'amp': 50., 'const': 1.}, \n",
    "              {'period': 20., 'phase': 0., 'amp': 5., 'const': 5.}]\n",
    "cls_wts = None # even split for now\n",
    "num_cls = len(cls_models)\n",
    "# will need a way to draw model params\n",
    "\n",
    "def_cadence = np.arange(0., 200., 5.)\n",
    "lcs = []\n",
    "truth = np.random.choice(range(num_cls), num_obj, p=cls_wts)\n",
    "ids, inds, cts = np.unique(truth, return_counts=True, return_inverse=True)\n",
    "# print(ids, cts, inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some lightcurves and record which are of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    times = make_cadence(def_cadence, 0.5)\n",
    "    model = cls_models[ids[inds[i]]](**cls_params[ids[inds[i]]])\n",
    "    phot, err = apply_err(model(times), 0.1)\n",
    "    lcs.append(LC(times, phot))\n",
    "    \n",
    "masks = np.zeros((num_cls, num_obj, num_obj))\n",
    "for i in ids:\n",
    "    which_ones = np.where(truth == i)[0]\n",
    "#     print(which_ones)\n",
    "    pairs = np.array(list(itertools.permutations(which_ones, 2))).T\n",
    "#     print(pairs)\n",
    "    masks[i, pairs[0], pairs[1]] += 1\n",
    "    \n",
    "# print(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop over the optimization, comparing pairwise.  We won't worry about skipping duplication yet because we can use it as a null test of whether this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_pipeline(all_lcs):\n",
    "    how_many = len(all_lcs)\n",
    "    indices = range(how_many)\n",
    "    dump_difs = np.empty((how_many, how_many))\n",
    "    dump_params = []\n",
    "    \n",
    "    for i in indices:\n",
    "        one_set = []\n",
    "        for j in indices:\n",
    "            ans, fin_len = find_max_prob(all_lcs[i], all_lcs[j])\n",
    "#             print(i, j, ans, fin_len)\n",
    "            one_set.append(np.asarray(ans.x))\n",
    "            dump_difs[i][j] = fin_len\n",
    "        dump_params.append(one_set)\n",
    "    dump_params = np.array(dump_params)\n",
    "            \n",
    "    return(dump_params, dump_difs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params, all_difs = mini_pipeline(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        print((i, j, all_difs[i][j]))\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        print((i, j, truth[i], truth[j]))\n",
    "        try_to_fit(merge(lcs[i], transform(lcs[j], all_params[i][j][0], all_params[i][j][1], all_params[i][j][2], all_params[i][j][3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for symmetry -- really thought these would be symmetric. . .\n",
    "plt.matshow(np.sum(masks, axis=0))\n",
    "layered = np.swapaxes(all_params, 0, -1)\n",
    "\n",
    "deltafunc = lambda x: np.abs(x)\n",
    "stretchfunc = lambda x: np.min(np.array([x, 1./x]).T, axis=-1)\n",
    "funcs = [deltafunc, deltafunc, stretchfunc, stretchfunc]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.matshow(funcs[i](layered[i]))\n",
    "    plt.plot([0, num_obj-1], [0, num_obj-1], color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, this doesn't seem to be working well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster in the space of summary statistics\n",
    "\n",
    "kdtree (and more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if the stretch/shear parameters for a class are clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mask = np.zeros((num_obj, num_obj))\n",
    "# for i in range(4):\n",
    "#     global_mask = np.logical_or(global_mask, masks[i])\n",
    "for i in range(num_cls):\n",
    "    global_mask = np.logical_or(global_mask, masks[i])\n",
    "    plt.hist((all_difs * masks[i]).flatten(), alpha=0.25, label=str(i))\n",
    "plt.hist(all_difs[~global_mask[i]].flatten(), alpha=0.25, label='no match')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of makes sense because we expect pairs of (s, 1/s) for stretch s and (t, -t) for translation t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(all_params.reshape(100, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, they really aren't symmetric nor interpretable.  But this is a very small sample size. . . so let's do it a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another approach :-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerize(data, masks):\n",
    "    datashape = np.shape(data)\n",
    "    global_mask = np.ma.make_mask_none(np.shape(masks)[1:])\n",
    "    layers = []\n",
    "    for i in range(len(masks)):# per class\n",
    "        one_mask = np.ma.make_mask(masks[i])\n",
    "        layer = np.ma.array(data, mask=np.ma.logical_not(one_mask)[np.newaxis])#data * masks[i][np.newaxis]\n",
    "        global_mask = np.ma.mask_or(global_mask, one_mask)\n",
    "        layers.append(layer.compressed())\n",
    "        \n",
    "    global_mask = np.ma.make_mask(global_mask)\n",
    "    others = np.ma.array(data, mask=global_mask[np.newaxis]).compressed()#data * ~global_mask[np.newaxis]\n",
    "    return(layers, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class, mismatch = listerize(all_difs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(m1, m2):\n",
    "    X, Y = np.mgrid[min(m1):max(m1):100j, min(m2):max(m2):100j]                                                     \n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])                                                       \n",
    "    values = np.vstack([m1, m2])                                                                        \n",
    "    kernel = sps.gaussian_kde(values)                                                             \n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "    return X, Y, Z\n",
    "\n",
    "def mycorner(data, keys, colors, maps, lims=None, pre_densities=None, filename='plot.pdf'):\n",
    "    ncol = len(keys)\n",
    "    fig = plt.figure(figsize=(ncol*5, ncol*5))\n",
    "    ax = [[fig.add_subplot(ncol, ncol, ncol * i + j + 1) for j in range(i+1)] for i in range(ncol)]\n",
    "#     print(len(data), len(colors))\n",
    "    for k in range(len(data)):\n",
    "        datum = data[k]\n",
    "        npoints = len(datum)\n",
    "        for i in range(ncol):\n",
    "            for j in range(i+1):\n",
    "                if i == j:\n",
    "#                     print(datum[keys[i]])\n",
    "                    ax[i][j].hist(datum[i].data, histtype='step', linewidth=2, alpha=0.5, color=colors[k])\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                else:\n",
    "#                     if (npoints >= 1e4 or npoints <= 100):\n",
    "                    ax[i][j].scatter(datum[i].data, datum[j].data, color=colors[k], alpha=0.5)\n",
    "#                     else:\n",
    "#                         if pre_densities is None:\n",
    "#                             x, y, z = density_estimation(datum[keys[i]], datum[keys[j]])\n",
    "#                         else:\n",
    "#                             (x, y, z) = pre_densities[i][j]\n",
    "#                         ax[i][j].contour(x, y, z, cmap=plt.get_cmap(maps[k]) , alpha=0.5)\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                    ax[i][j].set_ylabel(keys[j])\n",
    "#                     if lims is not None:\n",
    "#                         ax[i][j].set_xlim(lims)\n",
    "#                         ax[i][j].set_ylim(lims)\n",
    "#     fig.savefig(filename, dpi=100)\n",
    "    return#(fig)\n",
    "# replace with 2d histogram for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print per_class[0].shape, per_class[1].shape\n",
    "print len(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mycorner([per_class[0], per_class[1], mismatch], ['deltax', 'deltay', 'stretchx', 'stretchy'], ['r', 'g', 'b'], ['Reds', 'Greens', 'Blues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, this still doesn't look like anything. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ideas\n",
    "\n",
    "pairwise combinations/comparisons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space partitioning -- try to estimate\n",
    "\n",
    "$$\\int_{\\theta \\in D} p(x|\\theta) d\\theta$$\n",
    "\n",
    "then we can iteratively refine. maybe have coarse upper/lower bounds by sampling discrete $\\theta$ and multiplying by the volume of $D$, idk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have independent observations $y_i$ and times $x_i$, maybe we want the product probability that some underlying function $f$ generated those points\n",
    "\n",
    "$$\\prod_i p(y_i | \\theta)$$\n",
    "\n",
    "if the error distributions are just gaussians with standard deviation $\\sigma$ then the logprob is\n",
    "\n",
    "$$\\log \\left(\\prod_i \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{f(x_i)-y_i}{\\sigma}\\right)^2} \\right)$$\n",
    "\n",
    "which is for some constant $|C|$,\n",
    "\n",
    "$$-|C| \\sum_i \\left(f(x_i) - y_i\\right)^2$$\n",
    "\n",
    "if $f$ is differentiable like the sine function, i think we can get a closed-form solution for finding the min (closest parameters). we probably want something more bayesian, but i imagine the probabilities fall off kinda fast from the min point, so having it as a reference might be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as scipy_norm\n",
    "\n",
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf(x, 1, 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf((x - 1) / 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logsum_p_obs(expected, observed, noise_scale):\n",
    "#     diffs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
