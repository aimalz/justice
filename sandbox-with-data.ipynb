{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Alex Malz, David Mykytyn, Zora Tung_\n",
    "\n",
    "This is a sandbox for developing an unsupervised classifier of astronomical lightcurves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ongoing Ideas\n",
    "\n",
    "_Special thanks to David Hogg_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better metric\n",
    "\n",
    "- make it chisquare of arclength\n",
    "- arclength must be relative to reference to account for different noise levels, or arclength relative to sum of both originals, then inverse transformation for symmetry, relative to null of no-overlap\n",
    "\n",
    "complete sine/cosine basis -- rotation into space will look like linear fit, penalize high amplitude on high frequency modes --> probability, prior on frequency or number of modes, that would be a posterior probability\n",
    "\n",
    "linearly interpolate one and get probability of other under same model\n",
    "\n",
    "minimize 2nd derivative/sharpness\n",
    "\n",
    "try different noise levels\n",
    "\n",
    "try crazy outlier points, try arclength over all droppings of single point\n",
    "\n",
    "many bands -- share shift but stretch will differ, can make a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "- always looking at 3 year segment centered on overlap so never penalized by too long, enforce that there is overlap\n",
    "- prior to make product of stretches = 1\n",
    "\n",
    "chunks?\n",
    "\n",
    "trigger time? probably not provided\n",
    "\n",
    "regularizing erros: probably happens in space of variance, should be sensitive to shift in y, stretch in y, just need ratio of initial and final y values and initial variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "check symmetry requirements of distance metrics for clustering, i.e. tSNE needs convexity and symmetry\n",
    "\n",
    "hierarchical clustering by combining best fits, reverse merger tree \"active learning\", both way branching is logN so we can visually monitor as we add new things\n",
    "\n",
    "need a way to split out again, add one new thing, check against everything elses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import corner\n",
    "import george"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some mock data\n",
    "\n",
    "We may need to preprocess to keep it reasonable, constraints on delta/stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from justice import simulate as sim\n",
    "from justice import summarize as summ\n",
    "from justice import visualize as vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kinds of LC shapes might we have to worry about physically?  For now, just one transient (Gaussian) and one variable (sinusoid).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lightcurves are sampled irregularly/sparsely in x and have observational errors/noise on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, ready to use some mock data from SNPhotCC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_cadence = np.arange(0., 1000., 10.)\n",
    "\n",
    "# gmodel = sim.make_gauss(100., 300., 5., 1.)\n",
    "# gtimes = sim.make_cadence(def_cadence, 0.5)\n",
    "# gtrue = gmodel(gtimes)\n",
    "# gphot, gerr = sim.apply_err(gtrue, 0.1)\n",
    "\n",
    "# smodel = sim.make_sine(10., 0., 1., 2.)\n",
    "# stimes = sim.make_cadence(def_cadence, 0.5)\n",
    "# strue = smodel(stimes)\n",
    "# sphot, serr = sim.apply_err(strue, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justice import supernova_data\n",
    "from importlib import reload\n",
    "reload(supernova_data)\n",
    "\n",
    "sn_dataset = supernova_data.SNDataset()\n",
    "sn_dataset.index_df[sn_dataset.index_df['type'] != -9].groupby('type').count()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_choices = [1, 2, 3]\n",
    "per_type_ids = []\n",
    "for i in type_choices:\n",
    "    per_type_ids.append(sn_dataset.get_ids_for_type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_two_lcs_by_filter(id_array):\n",
    "    return [\n",
    "        supernova_data.data_by_flt(sn_dataset.get_lc_by_id(id_array[0])),\n",
    "        supernova_data.data_by_flt(sn_dataset.get_lc_by_id(id_array[1])),\n",
    "    ]\n",
    "\n",
    "noahs_ark = []\n",
    "for i in per_type_ids:\n",
    "    noahs_ark.append(first_two_lcs_by_filter(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lightcurve objects for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from justice import lightcurve\n",
    "\n",
    "all_lcs = []\n",
    "for t in range(len(type_choices)):\n",
    "    for n in range(2):\n",
    "#         split_up = np.hsplit(noahs_ark[t][n]['r'], 3)\n",
    "#         print(noahs_ark[t][n].keys())\n",
    "        up_to_r = noahs_ark[t][n]['r']\n",
    "        in_args = [up_to_r[:, i] for i in range(3)]\n",
    "        one_lc = lightcurve.LC(*in_args)\n",
    "#         print(np.hsplit(noahs_ark[t][n]['r'], 3))\n",
    "#         break\n",
    "#     break\n",
    "        all_lcs.append(one_lc)\n",
    "    vis.plot_lcs(all_lcs[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glc = sim.LC(gtimes, gphot, gerr)\n",
    "# slc = sim.LC(stimes, sphot, serr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.plot_lcs([glc, slc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The overall strategy\n",
    "\n",
    "We want to test the hypothesis that two lightcurves are noisy/sparse/irregular observations of the same object, under some permitted (afine) transformations.  Then we want to do clustering in the space of goodness-of-fit/consistency measure and the parameters of those transformations to identify classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularize in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regx(lca0, lcb0, lcc):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permitted transformations\n",
    "\n",
    "* shiftx\n",
    "* stretchx\n",
    "* shifty\n",
    "* stretchy\n",
    "* (cross-talk between bands)\n",
    "\n",
    "\n",
    "also adjust error bars\n",
    "\n",
    "should we first identify zero point in time to not have to worry as much about the arclength optimization? \n",
    "could we do that with fourier transform first?\n",
    "start with large grid, then refine grid, dayish scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce to summary statistics (consistency metric)\n",
    "\n",
    "Contenders:\n",
    "\n",
    "* periodogram -- identify periodicity and stochastic noise levels, still okay to initially divide transient from variable\n",
    "* flux per time bins -- trends keeping bin size constant but changing bin ends, i.e. moving window\n",
    "* abs/percent change in color and total flux/magnitude\n",
    "\n",
    "find MAP/MLE of p(A = B | lc_A, lc_B)\n",
    "marginalize over shift/stretch params\n",
    "\n",
    "Regularization is going to be really hard!\n",
    "\n",
    "connect the dots is taking an arc length\n",
    "\n",
    "could use the gaussian error bars to get probability that new hypothesis point lies on original line?\n",
    "\n",
    "Random ideas: gaussian process kernels based on training set, get probability that connect-the-dots is drawn from that distribution; but, we know training set will be biased/incomplete, so will have to be able to adapt kernel, use some kind of exporation of space of kernels (gradient descent, genetic algorithm, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the arclength.  If the two lightcurves came from the same object, then their arclengths should be comparable to one another (if over the same range).  Merging the lightcurves should not significantly affect the total arclength.  We could also consider an area between curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing an optimization to find the transformation parameters minimizing the arclength ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff = sim.Aff(50., 1., 1., 1.5)\n",
    "glc2 = sim.transform(glc, aff)\n",
    "\n",
    "aff = summ.opt_arclen(glc, glc2, vb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try just doing this with merging and shifting for now and test it when the lightcurves have the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis.plot_arclen_res(glc, glc2, aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glc3 = summ.merge(glc, sim.transform(glc2, aff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian processes as an alternative to the arclength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from george import kernels\n",
    "# help(kernels.ExpSquaredKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is definitely not the right kernel for the job!\n",
    "kernel = np.var(glc3.y) * kernels.ExpSquaredKernel(10.)\n",
    "lcf, fin_like = summ.fit_gp(kernel, glc3, def_cadence)\n",
    "\n",
    "vis.plot_gp_res(glc3, lcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broken after this point!  Will turn the rest into scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do this many times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obj = 10\n",
    "cls_models = [make_gauss, make_sine]\n",
    "cls_params = [{'scale': 10., 'loc': 100., 'amp': 50., 'const': 1.}, \n",
    "              {'period': 20., 'phase': 0., 'amp': 5., 'const': 5.}]\n",
    "cls_wts = None # even split for now\n",
    "num_cls = len(cls_models)\n",
    "# will need a way to draw model params\n",
    "\n",
    "def_cadence = np.arange(0., 200., 5.)\n",
    "lcs = []\n",
    "truth = np.random.choice(range(num_cls), num_obj, p=cls_wts)\n",
    "ids, inds, cts = np.unique(truth, return_counts=True, return_inverse=True)\n",
    "# print(ids, cts, inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some lightcurves and record which are of the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    times = make_cadence(def_cadence, 0.5)\n",
    "    model = cls_models[ids[inds[i]]](**cls_params[ids[inds[i]]])\n",
    "    phot, err = apply_err(model(times), 0.1)\n",
    "    lcs.append(LC(times, phot))\n",
    "    \n",
    "masks = np.zeros((num_cls, num_obj, num_obj))\n",
    "for i in ids:\n",
    "    which_ones = np.where(truth == i)[0]\n",
    "#     print(which_ones)\n",
    "    pairs = np.array(list(itertools.permutations(which_ones, 2))).T\n",
    "#     print(pairs)\n",
    "    masks[i, pairs[0], pairs[1]] += 1\n",
    "    \n",
    "# print(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's loop over the optimization, comparing pairwise.  We won't worry about skipping duplication yet because we can use it as a null test of whether this is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_pipeline(all_lcs):\n",
    "    how_many = len(all_lcs)\n",
    "    indices = range(how_many)\n",
    "    dump_difs = np.empty((how_many, how_many))\n",
    "    dump_params = []\n",
    "    \n",
    "    for i in indices:\n",
    "        one_set = []\n",
    "        for j in indices:\n",
    "            ans, fin_len = find_max_prob(all_lcs[i], all_lcs[j])\n",
    "#             print(i, j, ans, fin_len)\n",
    "            one_set.append(np.asarray(ans.x))\n",
    "            dump_difs[i][j] = fin_len\n",
    "        dump_params.append(one_set)\n",
    "    dump_params = np.array(dump_params)\n",
    "            \n",
    "    return(dump_params, dump_difs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params, all_difs = mini_pipeline(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        print((i, j, all_difs[i][j]))\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        print((i, j, truth[i], truth[j]))\n",
    "        try_to_fit(merge(lcs[i], transform(lcs[j], all_params[i][j][0], all_params[i][j][1], all_params[i][j][2], all_params[i][j][3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for symmetry -- really thought these would be symmetric. . .\n",
    "plt.matshow(np.sum(masks, axis=0))\n",
    "layered = np.swapaxes(all_params, 0, -1)\n",
    "\n",
    "deltafunc = lambda x: np.abs(x)\n",
    "stretchfunc = lambda x: np.min(np.array([x, 1./x]).T, axis=-1)\n",
    "funcs = [deltafunc, deltafunc, stretchfunc, stretchfunc]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.matshow(funcs[i](layered[i]))\n",
    "    plt.plot([0, num_obj-1], [0, num_obj-1], color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, this doesn't seem to be working well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster in the space of summary statistics\n",
    "\n",
    "kdtree (and more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if the stretch/shear parameters for a class are clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mask = np.zeros((num_obj, num_obj))\n",
    "# for i in range(4):\n",
    "#     global_mask = np.logical_or(global_mask, masks[i])\n",
    "for i in range(num_cls):\n",
    "    global_mask = np.logical_or(global_mask, masks[i])\n",
    "    plt.hist((all_difs * masks[i]).flatten(), alpha=0.25, label=str(i))\n",
    "plt.hist(all_difs[~global_mask[i]].flatten(), alpha=0.25, label='no match')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sort of makes sense because we expect pairs of (s, 1/s) for stretch s and (t, -t) for translation t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(all_params.reshape(100, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, they really aren't symmetric nor interpretable.  But this is a very small sample size. . . so let's do it a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another approach :-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listerize(data, masks):\n",
    "    datashape = np.shape(data)\n",
    "    global_mask = np.ma.make_mask_none(np.shape(masks)[1:])\n",
    "    layers = []\n",
    "    for i in range(len(masks)):# per class\n",
    "        one_mask = np.ma.make_mask(masks[i])\n",
    "        layer = np.ma.array(data, mask=np.ma.logical_not(one_mask)[np.newaxis])#data * masks[i][np.newaxis]\n",
    "        global_mask = np.ma.mask_or(global_mask, one_mask)\n",
    "        layers.append(layer.compressed())\n",
    "        \n",
    "    global_mask = np.ma.make_mask(global_mask)\n",
    "    others = np.ma.array(data, mask=global_mask[np.newaxis]).compressed()#data * ~global_mask[np.newaxis]\n",
    "    return(layers, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class, mismatch = listerize(all_difs, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_estimation(m1, m2):\n",
    "    X, Y = np.mgrid[min(m1):max(m1):100j, min(m2):max(m2):100j]                                                     \n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])                                                       \n",
    "    values = np.vstack([m1, m2])                                                                        \n",
    "    kernel = sps.gaussian_kde(values)                                                             \n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "    return X, Y, Z\n",
    "\n",
    "def mycorner(data, keys, colors, maps, lims=None, pre_densities=None, filename='plot.pdf'):\n",
    "    ncol = len(keys)\n",
    "    fig = plt.figure(figsize=(ncol*5, ncol*5))\n",
    "    ax = [[fig.add_subplot(ncol, ncol, ncol * i + j + 1) for j in range(i+1)] for i in range(ncol)]\n",
    "#     print(len(data), len(colors))\n",
    "    for k in range(len(data)):\n",
    "        datum = data[k]\n",
    "        npoints = len(datum)\n",
    "        for i in range(ncol):\n",
    "            for j in range(i+1):\n",
    "                if i == j:\n",
    "#                     print(datum[keys[i]])\n",
    "                    ax[i][j].hist(datum[i].data, histtype='step', linewidth=2, alpha=0.5, color=colors[k])\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                else:\n",
    "#                     if (npoints >= 1e4 or npoints <= 100):\n",
    "                    ax[i][j].scatter(datum[i].data, datum[j].data, color=colors[k], alpha=0.5)\n",
    "#                     else:\n",
    "#                         if pre_densities is None:\n",
    "#                             x, y, z = density_estimation(datum[keys[i]], datum[keys[j]])\n",
    "#                         else:\n",
    "#                             (x, y, z) = pre_densities[i][j]\n",
    "#                         ax[i][j].contour(x, y, z, cmap=plt.get_cmap(maps[k]) , alpha=0.5)\n",
    "                    ax[i][j].set_xlabel(keys[i])\n",
    "                    ax[i][j].set_ylabel(keys[j])\n",
    "#                     if lims is not None:\n",
    "#                         ax[i][j].set_xlim(lims)\n",
    "#                         ax[i][j].set_ylim(lims)\n",
    "#     fig.savefig(filename, dpi=100)\n",
    "    return#(fig)\n",
    "# replace with 2d histogram for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print per_class[0].shape, per_class[1].shape\n",
    "print len(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mycorner([per_class[0], per_class[1], mismatch], ['deltax', 'deltay', 'stretchx', 'stretchy'], ['r', 'g', 'b'], ['Reds', 'Greens', 'Blues'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, this still doesn't look like anything. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_obj):\n",
    "    for j in range(num_obj):\n",
    "        plot_reconstruct(lcs[i], lcs[j], all_params[i][j], truea=str(truth[i]), trueb=str(truth[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ideas\n",
    "\n",
    "pairwise combinations/comparisons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space partitioning -- try to estimate\n",
    "\n",
    "$$\\int_{\\theta \\in D} p(x|\\theta) d\\theta$$\n",
    "\n",
    "then we can iteratively refine. maybe have coarse upper/lower bounds by sampling discrete $\\theta$ and multiplying by the volume of $D$, idk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have independent observations $y_i$ and times $x_i$, maybe we want the product probability that some underlying function $f$ generated those points\n",
    "\n",
    "$$\\prod_i p(y_i | \\theta)$$\n",
    "\n",
    "if the error distributions are just gaussians with standard deviation $\\sigma$ then the logprob is\n",
    "\n",
    "$$\\log \\left(\\prod_i \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{f(x_i)-y_i}{\\sigma}\\right)^2} \\right)$$\n",
    "\n",
    "which is for some constant $|C|$,\n",
    "\n",
    "$$-|C| \\sum_i \\left(f(x_i) - y_i\\right)^2$$\n",
    "\n",
    "if $f$ is differentiable like the sine function, i think we can get a closed-form solution for finding the min (closest parameters). we probably want something more bayesian, but i imagine the probabilities fall off kinda fast from the min point, so having it as a reference might be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as scipy_norm\n",
    "\n",
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf(x, 1, 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(scipy_norm.ppf(0.01),\n",
    "                scipy_norm.ppf(0.99), 100)\n",
    "plt.plot(x, scipy_norm.pdf((x - 1) / 2),\n",
    "        'r-', lw=5, alpha=0.6, label='norm pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logsum_p_obs(expected, observed, noise_scale):\n",
    "#     diffs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
